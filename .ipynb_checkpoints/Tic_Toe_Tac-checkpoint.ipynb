{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIC TAE TOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542'>click me for refernece</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,eps=0.1,alpha=0.5,verbose=False):\n",
    "        self.eps = eps   #epsilon for greedy algo\n",
    "        self.alpha = alpha #learning rate for update\n",
    "        self.verbose = verbose #for extra information\n",
    "        self.state_history = list() #store the history after every action\n",
    "        \n",
    "    def set_symbol(self,s):   #function to set the symbol like 'o','x'\n",
    "        self.sym = s\n",
    "    \n",
    "    def set_value(self,v):    #store the reward of every state \n",
    "        self.V = v\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.state_history =  list()\n",
    "    \n",
    "    def take_action(self,env): # apply epsilon greedy algorithm\n",
    "   \n",
    "        r = np.random.rand()\n",
    "        best_state = None\n",
    "        \n",
    "        if r < self.eps:\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('Taking a random action !')\n",
    "            \n",
    "            possible_moves = list()\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i,j):\n",
    "                        possible_moves.append((i,j))\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            next_move =  possible_moves[idx]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            pos2val = dict()\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i,j):\n",
    "                        env.board[i,j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i,j] = 0\n",
    "                        pos2val[(i,j)] = self.V[state]\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i,j)\n",
    "        \n",
    "        \n",
    "        #if verbose is true we need to print the value of current board after taking action\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Taking a greedy action !')\n",
    "            \n",
    "            for i in range(LENGTH):\n",
    "                print(\"******************\")\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i,j):\n",
    "                        print('%.2f'%pos2val[(i,j)],end = \" \")\n",
    "                    else:\n",
    "                        print(\" \",end=\" \")\n",
    "                        if env.board[i,j] == env.o:\n",
    "                            print(\"x |\",end=\" \")\n",
    "                        elif env.board[i,j] == env.x:\n",
    "                            print(\"o |\",end=\" \")\n",
    "                        else:\n",
    "                            print(\" |\",end=\"\")\n",
    "                    print()\n",
    "                print(\"******************\")\n",
    "                \n",
    "            \n",
    "            env.board[next_move[0],next_move[1]] = self.sym\n",
    "        \n",
    "    def update_state_history(self,s):\n",
    "        self.state_history.append(s)\n",
    "    \n",
    "    def update(self,env): #we only use this function at end of episode\n",
    "        \n",
    "        target = env.reward(self.sym)\n",
    "        \n",
    "        for p in reversed(self.state_history):\n",
    "            \n",
    "            v = self.V[p] + self.alpha*(target-self.V[p]) #prev state + alpha*(nextstate-prevstate)\n",
    "            self.V[p] = v\n",
    "            target = v\n",
    "            \n",
    "        self.reset_history()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((LENGTH,LENGTH))\n",
    "        self.x = -1\n",
    "        self.o = +1\n",
    "        self.ended = False\n",
    "        self.winner = None\n",
    "        self.num_states = 3**(LENGTH*LENGTH)\n",
    "        \n",
    "    def is_empty(self,i,j):\n",
    "        return self.board[i][j] == 0\n",
    "    \n",
    "    def reward(self,sym):\n",
    "        \n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        if self.winner==sym :\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_state(self):\n",
    "        \n",
    "        k = 0\n",
    "        h = 0\n",
    "        v = -1\n",
    "        for i in range(LENGTH):\n",
    "            for j in range(LENGTH):\n",
    "                \n",
    "                if self.board[i,j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    v = 2\n",
    "                elif self.board[i,j] == 0:\n",
    "                    v = 0\n",
    "                    \n",
    "                h += (3**k)*v\n",
    "                k+=1\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def game_over(self,force_recalculate = False):\n",
    "        \n",
    "        if not force_recalculate and self.ended:\n",
    "            return True\n",
    "        \n",
    "        #rows winner\n",
    "        for i in range(LENGTH):\n",
    "            for player in (self.o,self.x):\n",
    "                s = self.board[i].sum()\n",
    "                if s == LENGTH*player:\n",
    "                    self.ended = True\n",
    "                    self.winner = player\n",
    "                    return True\n",
    "        \n",
    "        #column winner\n",
    "        for j in range(LENGTH):\n",
    "            for player in (self.o,self.x):\n",
    "                s = self.board[:,j].sum()\n",
    "                if s == player*LENGTH:\n",
    "                    self.ended = True\n",
    "                    self.winner = player\n",
    "                    return True\n",
    "        \n",
    "        # diagonal winner\n",
    "        for player in (self.o,self.x):\n",
    "            if self.board.trace() == LENGTH*player:\n",
    "                self.ended = True\n",
    "                self.winner = player\n",
    "                return True\n",
    "            if np.fliplr(self.board).trace() == LENGTH*player:\n",
    "                self.ended = True\n",
    "                self.winner = player\n",
    "                return True\n",
    "        #draw\n",
    "        if np.all((self.board==0)==False):\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "        \n",
    "        self.winner = None\n",
    "        return False\n",
    "    \n",
    "    def is_draw(self):\n",
    "        return self.ended and self.winner is None\n",
    "\n",
    "\n",
    "    def display_board(self): \n",
    "        for i in range(LENGTH):\n",
    "            print(\"************\")\n",
    "            for j in range(LENGTH):\n",
    "                print(\"  \", end=\"\")\n",
    "                if self.board[i,j] == self.x:\n",
    "                    print(\"x \", end=\"\")\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    print(\"o \", end=\"\")\n",
    "                else:\n",
    "                    print(\"  \", end=\"\")\n",
    "            print(\"\")\n",
    "            print(\"************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.all([[1,2],[0,3]]))\n",
    "# print(np.all(([[1,2],[0,3]])==0))\n",
    "# print(([[1,2],[0,3]])==0)\n",
    "# print((([[1,2],[0,3]])==0)==False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def set_symbol(self,s):\n",
    "        self.sym = s\n",
    "    def take_action(self,env):\n",
    "        while True:\n",
    "            x = input(\"enter the coordinate i,j for move !\")\n",
    "            i,j = move.split(',')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i,j):\n",
    "                env.board[i,j] = self.sym\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Get state , Is Game is Ended and Winner of Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_end_or_winner(env,i=0,j=0):\n",
    "    \n",
    "    possible_result = list()\n",
    "    \n",
    "    for v in (0,env.x,env.o): #try all three possible states\n",
    "        env.board[i,j] = v\n",
    "        if j == LENGTH-1:\n",
    "            if i == LENGTH-1: # all board is filled\n",
    "                state = env.get_state()\n",
    "                ended = env.game_over(force_recalculate=True)\n",
    "                winner = env.winner\n",
    "                possible_result.append((state,ended,winner))\n",
    "            else:\n",
    "                possible_result += get_state_end_or_winner(env,i+1,0)\n",
    "        else:\n",
    "            possible_result += get_state_end_or_winner(env,i,j+1)\n",
    "            \n",
    "    return possible_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play ALL Possible Game (It included all impossible permutation too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_end_or_winner_all(env,turn='x'):\n",
    "    \n",
    "    possible_result = list()\n",
    "    state = env.get_state()\n",
    "    ended = env.game_over(force_recalculate=True)\n",
    "    winner = env.winner\n",
    "    possible_result.append((state,ended,winner))\n",
    "    board_before = env.board.copy()\n",
    "    \n",
    "    if ended:\n",
    "        if winner is not None:\n",
    "            env.draw_board()\n",
    "            print(\"Winner:\", 'x' if winner == -1 else 'o')\n",
    "            print()\n",
    "            assert(np.all(board_before == env.board))\n",
    "    \n",
    "    if not ended:\n",
    "        if turn == 'x':\n",
    "            sym = env.x\n",
    "            next_sym = 'o'\n",
    "        if turn == 'o':\n",
    "            sym = env.o\n",
    "            next_sym = 'x'\n",
    "    \n",
    "    for i in range(LENGTH):\n",
    "        for j in range(LENGTH):\n",
    "            if env.is_empty(i,j):\n",
    "                env.board[i,j] = sym\n",
    "                possible_result+=get_state_end_or_winner_all(env,next_sym)\n",
    "                env.board[i,j] = 0\n",
    "            \n",
    "    return possible_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
